# ðŸ§  Skin Lesion Classification Pipeline Protocol v2 (Archived)

**Version**: 2.0  
**Date**: July 2025  
**Status**: Archived (content merged into `protocol.md`)  

## ðŸ“‹ Rules & Guidelines

### ðŸ”„ **Core Rules**
- **Keep `.gitignore`, `requirements.txt`, `pipeline_protocol.md`, and `common_errors.md` updated**
- **Archive previous versions**: Always create `pipeline_protocol_v<N>.md` before major changes
- **Document all data pipeline stages**: Raw â†’ Cleaned â†’ Processed â†’ Features
- **Track dataset statistics**: Log counts, experiment assignments, and validation at each stage
- **Maintain reproducible splits**: Use fixed seeds and stratification

### ðŸ“Š **Data Pipeline Rules**
- **Raw data validation**: Check â‰¥100 images per dataset before processing
- **Ground truth verification**: Ensure CSV files exist and are properly formatted
- **Image size validation**: Confirm 512Ã—512 dimensions after resizing
- **Feature extraction logging**: Track SAM2 encoder feature generation
- **Dataset experiment consistency**: 70/15/15 train/val/test within each experiment

### ðŸ§  **Model Training Rules**
- **Architecture consistency**: Use same MLP structure across all three heads
- **Class index mapping**: Handle class imbalance with proper index remapping
- **Checkpoint metadata**: Save comprehensive model info with dataset config
- **Performance tracking**: Log to both CSV and WandB for all metrics
- **Memory management**: Implement GPU cleanup between training runs

## âš–ï¸ Lesion Loss Experiments (exp1)

Date: Aug 2025

- **Goal**: Improve MLP2 (8-class lesion head) on exp1 by addressing class imbalance and testing alternative losses.
- **Scripts**: `training/train_exp5_multihead.py`, `training/train_parallel.py`
- **Runner**: `training/retrain_exp1_only.sh`
- **Config toggle**: `config.yaml` â†’ `lesion_loss_fn: "weighted_ce" | "focal" | "lmf"`

### Implemented losses
- **weighted_ce**: Standard `nn.CrossEntropyLoss(weight=class_weights)` with inverse-frequency weights computed from exp1 train split; printed and logged at startup. Baseline.
- **focal**: `FocalLoss(alpha=class_weights, gamma=2.0)`; emphasizes hard examples. Logged alpha/gamma.
- **lmf**: LDAM + Focal combined (LMF). File: `losses/lmf_loss.py`. Uses LDAM margins m_c âˆ 1/âˆš(n_c) and focal term (Î³=2.0). Combined as `Î±Â·LDAM + Î²Â·Focal` with Î±=0.5, Î²=0.5.

### What was tuned/changed
- Added lesion loss toggle parsing in both trainers; logs choice to console + WandB (`lesion_loss_fn`, and `focal_gamma` when applicable).
- Computed lesion class counts/weights from train dataset (masked) on startup; printed to console and saved in logs.
- For LMF, passed exp1 train class counts to `LMFLoss` (computed per run, no hardcoding).
- `retrain_exp1_only.sh` names runs/logs by loss: `${EXPERIMENT}_${mode}_8classes_${LESION_LOSS_FN}`.

### Where to find results
- **Local logs**:
  - Multi-head: `backend/models/multi/logs_multi_exp1_8classes_{weighted_ce|focal|lmf}.txt`
  - Parallel: `backend/models/parallel/logs_parallel_exp1_8classes_{weighted_ce|focal|lmf}.txt`
- **Local checkpoints**:
  - Multi-head: `backend/models/multi/{mlp1,mlp2,mlp3}.pt`
  - Parallel: `backend/models/parallel/{mlp1,mlp2,mlp3}.pt`
- **WandB project**: `loss_comparison_weighted_ce_vs_focal` (LMF runs are included under same project)
  - Example runs:
    - Weighted CE: `exp1_multi_8classes_weighted_ce`, `exp1_parallel_8classes_weighted_ce`
    - Focal: `exp1_multi_8classes_focal`, `exp1_parallel_8classes_focal`
    - LMF: `exp1_multi_8classes_lmf`, `exp1_parallel_8classes_lmf`

### Observed exp1 performance (quick sanity runs)
- Multi-head:
  - weighted_ce: lesion acc â‰ˆ 0.514, lesion F1-macro â‰ˆ 0.244
  - focal: lesion acc â‰ˆ 0.131, lesion F1-macro â‰ˆ 0.158
  - lmf: [see run; not listed here in console snapshot]
- Parallel:
  - weighted_ce: lesion acc â‰ˆ 0.495, lesion F1 â‰ˆ 0.544
  - focal: lesion acc â‰ˆ 0.018, lesion F1 â‰ˆ 0.002
  - lmf: lesion acc â‰ˆ 0.645, lesion F1 â‰ˆ 0.599 (test); val acc â‰ˆ 0.645

Notes:
- Weighted CE is a strong baseline; focal underperformed with current Î±/Î³ on this setup.
- LMF (LDAM+Focal) substantially improved lesion accuracy in the parallel architecture on exp1.


### ðŸ” **Evaluation Rules**
- **Comprehensive metrics**: Accuracy, F1, Precision, Recall, AUC
- **Confusion matrices**: Generate for all models and save to results/
- **Visualization**: T-SNE, UMAP embeddings, ROC curves
- **Error analysis**: Document common failure cases and edge cases

## ðŸ“Š Current Metadata CSV Stats (live)

- **File**: `data/metadata/metadata.csv`
- **Total images**: 67,332
- **Columns**: `image_name`, `csv_source`, `diagnosis_from_csv`, `unified_diagnosis`, `exp1`, `exp2`, `exp3`, `exp4`, `exp5`

### Unified diagnosis distribution (counts)

| Label | Count |
|-------|------:|
| UNKNOWN | 28,753 |
| NV | 20,561 |
| MEL | 6,431 |
| BCC | 4,298 |
| BKL | 3,507 |
| AKIEC | 1,446 |
| NOT_SKIN | 1,061 |
| SCC | 588 |
| VASC | 357 |
| DF | 330 |

### Experiment coverage (non-empty per experiment assignments)

- exp1: train 47,178 Â· val 10,092 Â· test 10,062 (non-empty 67,332)
- exp2: train 2,563 Â· val 549 Â· test 526 (non-empty 3,638)
- exp3: train 6,601 Â· val 1,372 Â· test 1,424 (non-empty 9,397)
- exp4: train 2,554 Â· val 553 Â· test 675 (non-empty 3,782)
- exp5: train 7,445 Â· val 1,625 Â· test 1,608 (non-empty 10,678)

Notes:
- Empty means rows without an exp assignment; non-empty is total âˆ’ empty.
- Counts reflect the latest CSV on disk at edit time.

## ðŸ§© Training Architectures: Separate vs Parallel

### Trained separately (three independent MLPs)
- **Idea**: Train `MLP1` (skin/not-skin), `MLP2` (5-class lesion type), and `MLP3` (benign/malignant) as separate models.
- **Data**: Each uses its own filtered dataset/splits; runs can be sequential or as separate jobs.
- **Training/outputs**: Independent loops, checkpoints, and logs; inference runs the three heads one after another on the same SAM features.
- **Pros**: Simpler, decoupled; easy to scale/ablate.  **Cons**: No shared representation; more storage/compute for three checkpoints.

### Trained in parallel (single multi-task model)
- **Idea**: One shared trunk over SAM features with three parallel heads (skin/not-skin, lesion type, benign/malignant).
- **Data/labeling**: Unified loader; per-sample label masking so a head contributes to loss only when its label exists.
- **Loss**: Weighted sum `L = w_skin*L_skin + w_lesion*L_lesion + w_bm*L_bm` with ignore-index/masks for missing labels.
- **Training/outputs**: Single loop/backbone, joint optimization; one forward pass returns all three predictions.
- **Pros**: Shared representations, faster inference; potential transfer across tasks.  **Cons**: More tuning (task weights, masking), tighter coupling.

## ðŸ—ï¸ Project Structure v2

```
new_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Original datasets
â”‚   â”‚   â”œâ”€â”€ isic2018/                 # ISIC 2018 Challenge
â”‚   â”‚   â”œâ”€â”€ isic2019/                 # ISIC 2019 Challenge  
â”‚   â”‚   â”œâ”€â”€ isic2020/                 # ISIC 2020 Challenge
â”‚   â”‚   â”œâ”€â”€ dtd/                      # Describable Textures Dataset
â”‚   â”‚   â”œâ”€â”€ imagenet_ood/             # ImageNet OOD samples
â”‚   â”‚   â””â”€â”€ csv/                      # Ground truth files
â”‚   â”œâ”€â”€ cleaned_resized/              # Preprocessed images
â”‚   â”‚   â”œâ”€â”€ isic2018_512/            # 512Ã—512 resized
â”‚   â”‚   â”œâ”€â”€ isic2019_512/            # 512Ã—512 resized
â”‚   â”‚   â”œâ”€â”€ isic2020_512/            # 512Ã—512 resized
â”‚   â”‚   â””â”€â”€ plausibility_check_512/   # OOD validation images
â”‚   â”‚       â”œâ”€â”€ dtd/
â”‚   â”‚       â””â”€â”€ imagenet_ood/
â”‚   â”œâ”€â”€ processed/                    # Feature extraction outputs
â”‚   â”‚   â”œâ”€â”€ sam_features/             # SAM2 encoder features
â”‚   â”‚   â”œâ”€â”€ metadata/                 # Dataset experiments and labels
â”‚   â”‚   â””â”€â”€ unified_augmented.csv    # Master dataset file
â”‚   â””â”€â”€ metadata/                     # Dataset statistics and logs
â”œâ”€â”€ models/                           # Model architectures
â”‚   â”œâ”€â”€ sam_encoder.py               # SAM2 feature extractor
â”‚   â”œâ”€â”€ skin_not_skin_head.py        # MLP1: Binary classifier
â”‚   â”œâ”€â”€ lesion_type_head.py          # MLP2: 5-class classifier
â”‚   â””â”€â”€ benign_malignant_head.py     # MLP3: Binary classifier
â”œâ”€â”€ training/                         # Training scripts
â”‚   â”œâ”€â”€ train_mlp.py                 # Unified training script
â”‚   â”œâ”€â”€ train_skin_not_skin.py       # MLP1 specific
â”‚   â”œâ”€â”€ train_lesion_type.py         # MLP2 specific
â”‚   â””â”€â”€ train_benign_malignant.py    # MLP3 specific
â”œâ”€â”€ evaluation/                       # Evaluation and analysis
â”‚   â”œâ”€â”€ evaluate_heads.py            # Comprehensive evaluation
â”‚   â”œâ”€â”€ visualize_predictions.py     # Prediction visualization
â”‚   â””â”€â”€ analyze_performance.py       # Performance analysis
â”œâ”€â”€ utils/                           # Utilities and helpers
â”‚   â”œâ”€â”€ data_split.py               # Dataset experiment splitting logic
â”‚   â”œâ”€â”€ feature_extraction.py       # SAM2 feature extraction
â”‚   â”œâ”€â”€ metrics.py                  # Evaluation metrics
â”‚   â””â”€â”€ preview_datasets.py         # Dataset preview tools
â”œâ”€â”€ configs/                         # Configuration files
â”‚   â””â”€â”€ config.yaml                 # Main configuration
â”œâ”€â”€ saved_models/                    # Trained model checkpoints
â”‚   â”œâ”€â”€ skin_not_skin/              # MLP1 models
â”‚   â”œâ”€â”€ lesion_type/                # MLP2 models
â”‚   â””â”€â”€ benign_malignant/           # MLP3 models
â”œâ”€â”€ results/                         # Evaluation results
â”‚   â”œâ”€â”€ confusion_matrices/         # Confusion matrix plots
â”‚   â”œâ”€â”€ auc_curves/                 # ROC curve plots
â”‚   â”œâ”€â”€ tsne_umap_embeddings/       # Dimensionality reduction
â”‚   â””â”€â”€ dataset_stats/              # Dataset statistics
â”œâ”€â”€ logs/                           # Training logs
â”‚   â”œâ”€â”€ skin_not_skin/              # MLP1 training logs
â”‚   â”œâ”€â”€ lesion_type/                # MLP2 training logs
â”‚   â””â”€â”€ benign_malignant/           # MLP3 training logs
â”œâ”€â”€ nohup/                          # Background job logs
â”œâ”€â”€ wandb/                          # Weights & Biases logs
â””â”€â”€ docs/                           # Documentation
    â”œâ”€â”€ pipeline_protocol_v1.md     # Archived v1 protocol
    â””â”€â”€ common_errors.md            # Error solutions
```

## ðŸ“Š Data Pipeline Status v2

### âœ… **Current Status (from v1)**
- **Raw Data**: 89,397 images across ISIC datasets âœ…
- **Cleaned Images**: All resized to 512Ã—512 âœ…
- **Model Training**: All three MLPs completed âœ…
- **Performance**: MLP1 (99.38%), MLP2 (65.8%), MLP3 (73.12%) âœ…

- ### âœ… **NEW: Experiment System** âœ…
- **Unified Labels**: 67,332 images with 10 classes âœ…
- **5 Experiments**: Created for different experimental scenarios âœ…
- **Experiment Statistics**: Comprehensive distribution analysis âœ…

### âœ… **Issues Resolved (from earlier)**
- **Ground Truth CSVs**: Located and validated âœ…
- **OOD Datasets**: DTD and ImageNet OOD integrated âœ…
- **Data Pipeline Foundations**: Unified dataset class, stratified experiment assignments, and metadata completed âœ…

### ðŸŽ¯ **v2 Goals**
- **Complete Data Pipeline**: Raw â†’ Cleaned â†’ Features â†’ Training
- **Unified Dataset System**: Single dataset class for all models
- **Reproducible Splits**: Consistent 70/15/15 train/val/test within each experiment
- **Comprehensive Evaluation**: Full pipeline testing and analysis

## ðŸŽ¯ **Experiment System v2**

### ðŸ“Š **Dataset Overview**
- **Total Images**: 67,332
- **Classes**: 10 unified labels
- **File**: `data/metadata/metadata.csv`

### ðŸ·ï¸ **Class Distribution**
| Class | Count | Percentage |
|-------|-------|------------|
| other | 29,173 | 43.8% |
| nevus | 20,563 | 30.8% |
| melanoma | 6,433 | 9.6% |
| bcc | 4,298 | 6.4% |
| bkl | 3,507 | 5.3% |
| ak | 1,446 | 2.2% |
| scc | 588 | 0.9% |
| vascular | 357 | 0.5% |
| df | 330 | 0.5% |

### ðŸ”„ **Experiment Configurations**

#### **Experiment 1: Full Dataset** ðŸ“ˆ
- **Purpose**: Complete dataset baseline
- **Selection**: All 67,332 images
- **Use Case**: Maximum data utilization experiments
- **Samples**: 67,332 (100%)

#### **Experiment 2: Balanced Subset** âš–ï¸
- **Purpose**: Class-balanced experiments
- **Selection**: Max 2,000 samples per class
- **Logic**: Random sampling for classes >2,000, all samples for smaller classes
- **Samples**: 3,638
- **Distribution**: 
  - Large classes: 2,000 each (bcc, nevus, melanoma, bkl, other)
  - Small classes: All samples (ak: 1,446, scc: 588, vascular: 357, df: 330)

#### **Experiment 3: Rare-First Balancing** ðŸŽ¯
- **Purpose**: Focus on rare classes
- **Selection**: All rare classes + balanced sampling
- **Logic**: Include all rare classes (<1,000), sample others to match minimum rare count
- **Samples**: 9,397
- **Rare Classes**: scc, vascular, df (all included)
- **Balanced Count**: 330 samples per class (minimum rare class count)

#### **Experiment 4: Maximize Minorities** ðŸ“Š
- **Purpose**: Prioritize minority classes
- **Selection**: All minorities + 30% of majorities
- **Logic**: Include all classes <1,500, sample 30% from larger classes
- **Samples**: 3,782
- **Minority Classes**: All included (ak, scc, vascular, df)
- **Majority Classes**: 30% sampled (other, nevus, melanoma, bcc, bkl)

#### **Experiment 5: Small Classes Only** ðŸŽ¯
- **Purpose**: Focus on underrepresented classes
- **Selection**: Only classes with <1,500 images
- **Logic**: Exclude large classes entirely
- **Samples**: 10,678
- **Included Classes**: ak, scc, vascular, df
- **Excluded Classes**: other, nevus, melanoma, bcc, bkl

### ðŸ”§ **Technical Implementation**

#### **File Structure**
```
data/metadata/
â””â”€â”€ metadata.csv                         # Live metadata with experiment columns exp1..exp5
```

#### **Column Schema**
| Column | Type | Description |
|--------|------|-------------|
| image | string | Image filename |
| diagnosis | string | Original diagnosis |
| unified_label | string | Standardized class label |
| source_csv | string | Source dataset |
| exp1 | string | Assignment: train/val/test or empty |
| exp2 | string | Assignment: train/val/test or empty |
| exp3 | string | Assignment: train/val/test or empty |
| exp4 | string | Assignment: train/val/test or empty |
| exp5 | string | Assignment: train/val/test or empty |

#### **Usage Examples**
```python
# Load live metadata
df = pd.read_csv('data/metadata/metadata.csv')

# Filter for a specific experiment (non-empty assignment)
exp2_data = df[df['exp2'].astype(str) != '']
exp3_data = df[df['exp3'].astype(str) != '']

# Multi-experiment analysis
multi_exp = df[(df['exp2'].astype(str) != '') | (df['exp3'].astype(str) != '')]
```

### ðŸ“ˆ **Experimental Design**

#### **Experiment Selection Strategy**
- **Experiment 1**: Baseline experiments with full dataset
- **Experiment 2**: Balanced training experiments
- **Experiment 3**: Rare class focus experiments
- **Experiment 4**: Minority class optimization
- **Experiment 5**: Small class specialization

#### **Cross-Experiment Analysis**
- **Overlap Analysis**: Images can belong to multiple experiments
- **Performance Comparison**: Compare models across different experiments
- **Class Distribution Impact**: Study effect of class balancing
- **Data Efficiency**: Analyze performance vs. dataset size

### ðŸŽ¯ **Integration with Training Pipeline**

#### **Experiment-Aware Training**
```python
# Example training configuration
config = {
    'experiment_id': 2,  # Use experiment2 for balanced training
    'max_samples_per_class': 2000,
    'stratification': True,
    'random_seed': 42
}
```

#### **Experiment-Specific Metrics**
- **Per-Experiment Performance**: Track metrics for each experiment
- **Cross-Experiment Validation**: Validate across experiments
- **Experiment Comparison**: Compare model performance across experiments
- **Ensemble Methods**: Combine models from different experiments

### ðŸ“Š **Detailed Experiment Statistics & Source Dataset Analysis**

#### **Source Datasets Overview**
The unified dataset combines images from 6 different sources:

| Source Dataset | Total Images | Description |
|----------------|:-------------:|-------------|
| **ISIC_2020_Training_GroundTruth.csv** | 32,702 | ISIC 2020 Challenge training data |
| **ISIC_2019_Training_GroundTruth.csv** | 15,316 | ISIC 2019 Challenge training data |
| **ISIC2018_Task3_Training_GroundTruth.csv** | 10,015 | ISIC 2018 Challenge training data |
| **ISIC_2019_Test_GroundTruth.csv** | 8,045 | ISIC 2019 Challenge test data |
| **plausibility_check_512** | 1,061 | Out-of-distribution textures (ImageNet validation) |
| **ISIC2018_Task3_Validation_GroundTruth.csv** | 193 | ISIC 2018 Challenge validation data |

#### **Enhanced Experiment Statistics with Source Breakdown**

**experiment1 (Full Dataset - 67,332 images):**
- **Train**: 47,178 images (70.0%)
- **Val**: 10,092 images (15.0%)
- **Test**: 10,062 images (15.0%)

**experiment2 (Balanced Subset - 3,638 images):**
- **Train**: 2,563 images (70.4%)
- **Val**: 549 images (15.1%)
- **Test**: 526 images (14.5%)

**experiment3 (Rare-First Balancing - 9,397 images):**
- **Train**: 6,601 images (70.2%)
- **Val**: 1,372 images (14.6%)
- **Test**: 1,424 images (15.2%)

**experiment4 (Maximize Minorities - 3,782 images):**
- **Train**: 2,554 images (67.5%)
- **Val**: 553 images (14.6%)
- **Test**: 675 images (17.8%)

**experiment5 (Small Classes Only - 10,678 images):**
- **Train**: 7,445 images (69.7%)
- **Val**: 1,625 images (15.2%)
- **Test**: 1,608 images (15.1%)

#### **Label Distribution by Source Dataset**

**ISIC_2020_Training_GroundTruth.csv:**
- Primarily contains `other` (27,126) and `nevus` (5,193)
- Focus: Large-scale training data with diverse lesion types

**ISIC_2019_Training_GroundTruth.csv:**
- Balanced mix of all lesion types
- Strong representation of `melanoma` (3,409), `nevus` (6,170), `bcc` (2,809)

**ISIC_2019_Test_GroundTruth.csv:**
- Good representation of all classes
- Contains `other` (2,047) and `nevus` (2,372) prominently

**ISIC2018_Task3_Training_GroundTruth.csv:**
- Strong in `nevus` (6,705) and `melanoma` (1,113)
- Classic ISIC challenge dataset

**imagenet_ood:**
- Contains only `non_skin` (1,061) - out-of-distribution textures
- Used for robustness testing and OOD detection

**ISIC2018_Task3_Validation_GroundTruth.csv:**
- Small validation set with all lesion types
- Used for initial validation during development

#### **Stratified Experiment Implementation**
- **Method**: 70/15/15 train/val/test per experiment with stratification by `unified_diagnosis`
- **Random Seed**: 42 for reproducibility
- **Stratification**: Ensures proportional representation of all classes in each set
- **File**: `data/metadata/metadata.csv` (live)

#### **Generated Files**
- (archived) `data/metadata/unified_labels_with_splits.csv`: Original file with binary split flags (0/1)
- (archived) `data/metadata/unified_labels_with_stratified_splits.csv`: Enhanced file with stratified assignments ('train'/'val'/'test')
- (archived) `create_stratified_splits.py`: Script for creating earlier split files
- (archived) `count_splits.py`: Script for counting images in each split column

## ðŸš€ Pipeline Flow v2

```
Input Image
â†“
SAM2 Encoder (frozen) â†’ 256-dim features
â†“
MLP1: Skin/Not Skin Classification
â”œâ”€ If Not Skin â†’ STOP
â””â”€ If Skin â†’
    â†’ MLP2: Lesion Type Classification (5 classes)
    â†’ MLP3: Benign/Malignant Classification (2 classes)
â†“
Generate 3 SAM2 masks
â†“
Apply visualization heuristics
```

## ðŸ§  Model Architecture v2

### **Unified MLP Architecture**
```python
# [TO BE DEFINED]
```

### **Model Configurations**
| Config | Architecture | Dropout | Use Case |
|--------|-------------|---------|----------|
| [TO BE DEFINED] | [TO BE DEFINED] | [TO BE DEFINED] | [TO BE DEFINED] |

## ðŸ“‹ Dataset Configurations v2

| Config | Model | Description | Max Samples/Class | Augmented | OOD Included |
|--------|-------|-------------|:------------------:|:---------:|:------------:|
| [TO BE DEFINED] | [TO BE DEFINED] | [TO BE DEFINED] | [TO BE DEFINED] | [TO BE DEFINED] | [TO BE DEFINED] |

## ðŸ”§ Development Workflow v2

### **Data Pipeline Stages**
1. **Raw Data Validation** â†’ Check image counts and ground truth files
2. **Image Preprocessing** â†’ Resize to 512Ã—512, validate dimensions
3. **Feature Extraction** â†’ Generate SAM2 features for all images

   #### SAM Feature Extraction Protocol (v2, July 2025)
   - **Scope**: Features are extracted for all images listed in `data/metadata/metadata.csv` (67,332 images).
   - **Batch Size**: Dynamically set (currently 2) to avoid CUDA out-of-memory errors on 11GB GPUs. Adjust as needed for hardware.
   - **Device**: CUDA (GPU) is used if available; falls back to CPU if not.
   - **Incremental Saving**: Features and progress metadata are saved incrementally to `data/processed/features/sam_features_all_incremental.pkl` and `sam_features_all_incremental_metadata.pkl` every 100 batches (or every batch if batch_size=1).
   - **Resume Support**: Extraction can be resumed from the last saved batch in case of interruption.
   - **Logging**: Progress, errors (e.g., CUDA OOM), and batch completions are logged to console and file.
   - **Output**: Final features are stored as a pickle file, keyed by image name, for downstream model training.
   - **Status (July 2025)**: Extraction is in progress for all images. Previous runs were limited to the train partition; this is now corrected. As of last check, 67,332 images are being processed, with progress tracked in the metadata file.

4. **Dataset Splitting** â†’ Create 70/15/15 splits with stratification
5. **Model Training** â†’ Train all three MLPs with unified architecture
6. **Evaluation** â†’ Comprehensive performance analysis

### **Quality Assurance**
- **Data Validation**: Automated checks for image counts, dimensions, labels
- **Split Consistency**: Fixed random seeds for reproducible splits
- **Performance Tracking**: Comprehensive metrics logging
- **Error Handling**: Graceful failure recovery and logging

## ðŸ“ Version History

### **v1 (Archived)**
- âœ… Completed all three MLP training
- âœ… Achieved production-ready performance
- âœ… Implemented class index mapping
- âœ… Created comprehensive documentation

### **v2 (Current)**
- ðŸ”„ **Planning Phase**: Setting up complete data pipeline
- ðŸ”„ **Data Pipeline**: Raw â†’ Cleaned â†’ Features â†’ Training
- ðŸ”„ **Unified System**: Single dataset class and training script
- ðŸ”„ **Comprehensive Evaluation**: Full pipeline testing

---

## ðŸ“Š **TODO & PROGRESS v2**

### âœ… **COMPLETED (v1 Achievements)**

#### **Model Training - EXCELLENT** âœ…
- **MLP1 (Skin/Not-Skin)**: 99.38% validation accuracy âœ…
- **MLP2 (Lesion Type)**: 65.8% validation accuracy âœ…
- **MLP3 (Benign/Malignant)**: 73.12% validation accuracy âœ…
- **All models**: Production-ready with comprehensive checkpoints âœ…

#### **Data Processing - EXCELLENT** âœ…
- **Raw Data**: 89,397 images across ISIC datasets âœ…
  - ISIC 2018: 11,720 images âœ…
  - ISIC 2019: 33,569 images âœ…
  - ISIC 2020: 44,108 images âœ…
- **Cleaned Images**: All resized to 512Ã—512 âœ…
  - ISIC 2018: 11,720 images (512Ã—512) âœ…
  - ISIC 2019: 33,569 images (512Ã—512) âœ…
  - ISIC 2020: 44,108 images (512Ã—512) âœ…
- **Image Validation**: All dimensions confirmed 512Ã—512 âœ…

#### **Infrastructure - COMPLETE** âœ…
- **Project Structure**: Well-organized directory structure âœ…
- **Training Scripts**: All MLP training scripts functional âœ…
- **Model Checkpoints**: Saved with comprehensive metadata âœ…
- **Logging System**: WandB integration and CSV logging âœ…

#### **NEW: Experiment System - COMPLETE** âœ…
- **Unified Dataset**: 67,332 images with 10 classes âœ…
- **5 Experiments**: All experiments created and validated âœ…
- **Experiment Statistics**: Comprehensive analysis completed âœ…
- **Stratified Assignments**: 70/15/15 train/val/test with stratification âœ…
- **Source Dataset Analysis**: Detailed breakdown by source dataset âœ…
- **Visualization Suite**: Complete plotting and graphing system âœ…
- **Files**: 
  - `metadata.csv` (experiment assignments exp1..exp5) âœ…
  - `visualize_dataset_statistics.py` (visualization script) âœ…
- **Generated Plots**: Updated experiment/stratification visualizations âœ…

#### **NEW: Dataset Visualization System - COMPLETE** âœ…
- **Source Distribution Plots**: Pie charts and bar charts showing dataset contributions âœ…
- **Label Distribution Analysis**: Overall and source-specific label breakdowns âœ…
- **Experiment Analysis Visualizations**: Overlap matrices and distribution plots âœ…
- **Stratified Experiment Plots**: Train/val/test proportions for each experiment âœ…
- **Comprehensive Summary Dashboard**: All-in-one visualization with statistics âœ…
- **Generated Files**:
  - `source_distribution.png` - Source dataset breakdown
  - `label_distribution.png` - Label analysis across sources
  - `experiment_analysis.png` - Experiment overlap and distribution
  - `stratified_experiments.png` - Stratified assignment visualization
  - `comprehensive_summary.png` - Complete dashboard
- **Statistics Integration**: All visualizations saved to protocol documentation âœ…

#### **Key Dataset Statistics Summary** ðŸ“Š
- **Total Images on Disk**: 90,034 (including duplicates)
- **Unique Images on Disk**: 78,314 (after removing duplicates)
- **Images in CSV**: 67,332 (labeled images for training)
- **Missing from CSV**: 10,558 unique images
  - ISIC2020 test images: ~10,982 (unlabeled test data)
  - ImageNet validation: 1,061 (OOD validation images)
- **Source Datasets**: 6 (ISIC 2018-2020, DTD, ImageNet OOD)
- **Labels**: 10 classes (nevus, melanoma, bcc, bkl, ak, scc, df, vascular, other, non_skin)
- **Largest Source**: ISIC_2020_Training_GroundTruth.csv (33,126 images, 48.9%)
- **Experiment Coverage**:
  - exp1: 67,332 images (train 47,178 Â· val 10,092 Â· test 10,062)
  - exp2: 3,638 images (train 2,563 Â· val 549 Â· test 526)
  - exp3: 9,397 images (train 6,601 Â· val 1,372 Â· test 1,424)
  - exp4: 3,782 images (train 2,554 Â· val 553 Â· test 675)
  - exp5: 10,678 images (train 7,445 Â· val 1,625 Â· test 1,608)
- **Stratification**: 70/15/15 train/val/test with proper class balance âœ…
- **Directory Breakdown**:
  - ISIC2018: 11,720 images
  - ISIC2019: 33,569 images
  - ISIC2020: 43,684 images
  - Plausibility check: 1,061 images (ImageNet OOD)

### âŒ **CRITICAL ISSUES (v2 Priority)**

#### **Ground Truth CSVs** â€” Resolved âœ…
- `raw/isic2018/GroundTruth.csv`: âœ…
- `raw/isic2019/GroundTruth.csv`: âœ…
- `raw/isic2020/train.csv`: âœ…
- `raw/isic2020/duplicates.csv`: âœ…
- **Impact**: Unblocked training
- **Priority**: Cleared

#### **Out-of-Distribution Data (DTD, ImageNet OOD)** â€” Resolved âœ…
- `raw/dtd/`: âœ…
- `raw/imagenet_ood/`: âœ…
- `cleaned_resized/plausibility_check_512/dtd/`: âœ…
- `cleaned_resized/plausibility_check_512/imagenet_ood/`: âœ…
- **Impact**: MLP1 training unblocked
- **Priority**: Cleared

#### **Data Pipeline** â€” Updated Status
- **Feature Extraction**: In progress for all 67,756 images (incremental saving and resume enabled) ðŸ”„
- **Unified Dataset**: Implemented âœ…
- **Dataset Splits**: 70/15/15 with stratification implemented âœ…
- **Metadata & Statistics**: Comprehensive analysis completed âœ…
- **Priority**: **MEDIUM**

### ðŸ”„ **IN PROGRESS (v2 Development)**

#### **Planning & Documentation** ðŸ”„
- âœ… **Pipeline Protocol v2**: Created with updated structure
- âœ… **Data Pipeline Analysis**: Completed status assessment
- ðŸ”„ **Plan Development**: In progress (this document)
- ðŸ”„ **Task Prioritization**: Defining critical path

### ðŸŽ¯ **NEXT STEPS (v2 Roadmap)**

#### **Phase 1: Critical Data Issues** ðŸš¨
1. **Locate Ground Truth CSVs** - Search existing directories
2. **Download DTD Dataset** - Required for MLP1 training
3. **Download ImageNet OOD** - Required for MLP1 training
4. **Verify Data Integrity** - Validate all datasets
5. **âœ… Experiments Ready** - Use `data/metadata/metadata.csv` (exp1..exp5) for training

#### **Phase 2: Data Pipeline Completion** ðŸ“Š
1. **Implement Feature Extraction** - SAM2 encoder for all images
2. **Create Unified Dataset** - Single dataset class for all models
3. **Generate Dataset Splits** - 70/15/15 with stratification
4. **Validate Pipeline** - End-to-end testing

#### **Phase 3: Model Retraining** ðŸ§ 
1. **Retrain MLP1** - With complete OOD data
2. **Retrain MLP2** - With unified dataset system
3. **Retrain MLP3** - With all configurations
4. **Performance Validation** - Compare with v1 results

#### **Phase 4: Evaluation & Deployment** ðŸš€
1. **Comprehensive Evaluation** - All metrics and visualizations
2. **Pipeline Integration** - End-to-end testing
3. **Performance Optimization** - Memory and speed improvements
4. **Documentation** - Final v2 documentation

### ðŸ“ˆ **SUCCESS METRICS**

#### **Data Pipeline** ðŸ“Š
- [ ] **Ground Truth CSVs**: All 4 files located/downloaded
- [ ] **OOD Datasets**: DTD and ImageNet OOD added
- [ ] **Feature Extraction**: SAM2 features for all 67,756 labeled images in CSV (in progress)
- [x] **Dataset Splits**: 70/15/15 splits with proper stratification âœ…
- [x] **Unified Dataset**: Single dataset class functional âœ…
- [x] **Dataset Statistics**: Comprehensive analysis and visualization âœ…


#### **Code Quality** ðŸ”§
- [ ] **Unified Training Script**: Single script for all models
- [ ] **Comprehensive Logging**: All metrics tracked
- [ ] **Error Handling**: Graceful failure recovery
- [ ] **Documentation**: Complete v2 documentation

---

## ðŸŽ¯ **PLAN v2**

**[TO BE FILLED IN]**

### **Phase 1: Data Pipeline Setup**
- [ ] **Locate/Download Ground Truth CSVs**
- [ ] **Add DTD and ImageNet OOD Datasets**
- [ ] **Implement Feature Extraction Pipeline**
- [ ] **Create Unified Dataset System**

### **Phase 2: Model Training**
- [ ] **Train MLP1 with Complete OOD Data**
- [ ] **Train MLP2 with Unified Dataset**
- [ ] **Train MLP3 with All Configurations**
- [ ] **Validate All Model Performance**

### **Phase 3: Evaluation & Deployment**
- [ ] **Comprehensive Model Evaluation**
- [ ] **Pipeline Integration Testing**
- [ ] **Performance Optimization**
- [ ] **Documentation & Deployment**

---

**Next Steps**: Fill in the detailed plan based on current data pipeline status and requirements.  